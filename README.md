Building and training my first bigram transformer nueral network. This is a decoder-only model, that does not explicitly answer questions, rather it autocompletes Shakespeare. This is because I skipped out on the finetuning stages after the model was trained. 

This is an amazing video I used to learn about how to actually implement a generative pre-trained transformer.
https://www.youtube.com/watch?v=kCc8FmEb1nY

The research paper that started everything:
https://arxiv.org/pdf/1706.03762

A diagram of the decoder only architecture from Transformers:
![image](https://github.com/user-attachments/assets/860fad93-fba3-48bd-a0f2-0c1eb979c7cd)

